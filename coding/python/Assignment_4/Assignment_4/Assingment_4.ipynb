{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assingment_4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3z9l7S6RV8-2"
      },
      "source": [
        "In this assignment you will get a chance to implement LSTM cell from Scratch and Usage of Recurrent Neural Network for 1D time series Prediction task .\n",
        "\n",
        "#### **Instructions**\n",
        "1. Use Python 3.x to run this notebook\n",
        "3. Write your code only in between the lines 'YOUR CODE STARTS HERE' and 'YOUR CODE ENDS HERE'.\n",
        "you should not change anything else code cells, if you do, the answers you are supposed to get at the end of this assignment might be wrong.\n",
        "4. Read documentation of each function carefully."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT8skzfOLu91"
      },
      "source": [
        "## Question 1:\n",
        "\n",
        "Given a sequence of values of a 1D input time series from time t = 1 to t = 5, predict the value of the time series at t = 6 using RNN.\n",
        "\n",
        "Here we trained an RNN in such a way that, given values of input time series from t = 1 to t=i ; it will predict the value at t= i+1.\n",
        "\n",
        "Hint : Design an RNN using pytorch's nn.RNN to create an RNN layer , then add a fully-connected layer to get the required output size.\n",
        "\n",
        "Choose 32 as the number of features in the RNN output and in the hidden state. Also, choose number of layers to be 1 to make up the RNN, typically such number varies depending on different tasks. The value greater than 1 means that you'll create a stacked RNN. Also, use \"batch_first =True\". Here, \"batch_first\" implies whether or not the input/output of the RNN will have the batch_size as the first dimension (batch_size, seq_length, hidden_dim). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoVHp8ZWqYpd"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "## Fixing the seed for Reproducibility\n",
        "np.random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "\n",
        "## Define 1D input time series, which spans from t= 1 to t=6.\n",
        "input_series = np.random.randn(6,1)\n",
        "\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        ### YOUR CODE STARTS HERE\n",
        "        self.rnn = nn.RNN(input_size=input_size,hidden_size=32,num_layers=1,batch_first=True)\n",
        "        self.fc1 = nn.Linear(32,output_size)\n",
        "        \n",
        "    def forward(self, x, hidden):\n",
        "        out,hid=self.rnn(x)\n",
        "        out=self.fc1(out)\n",
        "        #out = self.fc1(out[:, -1, :]) \n",
        "        return out[0] ,hid\n",
        "        \n",
        "        ### YOUR CODE ENDS HERE\n",
        "\n",
        "# decide on hyperparameters\n",
        "input_size=1    ## 1D input\n",
        "output_size=1   ## 1D output\n",
        "hidden_dim=32  ## Hidden state feature dimension of RNN\n",
        "n_layers=1     ## No. of stacked layers in RNN\n",
        "\n",
        "# instantiate an RNN\n",
        "rnn = RNN(input_size, output_size, hidden_dim, n_layers)\n",
        "\n",
        "# MSE loss and Adam optimizer with a learning rate of 0.01\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.01)\n",
        "\n",
        "# train the RNN\n",
        "def train(rnn, n_steps, print_every):\n",
        "    \n",
        "    # initialize the hidden state\n",
        "    hidden = None      \n",
        "    \n",
        "    for batch_i, step in enumerate(range(n_steps)):\n",
        "        # defining the training data \n",
        "        x = input_series[:-1]\n",
        "        y = input_series[1:]\n",
        "        \n",
        "        # convert data into Tensors\n",
        "        x_tensor = torch.Tensor(x).unsqueeze(0) # unsqueeze gives a 1, batch_size dimension\n",
        "        y_tensor = torch.Tensor(y)\n",
        "\n",
        "        # outputs from the rnn\n",
        "        prediction, hidden = rnn(x_tensor, hidden)\n",
        "\n",
        "        ## Representing Memory ##\n",
        "        # make a new variable for hidden and detach the hidden state from its history\n",
        "        # this way, we don't backpropagate through the entire history\n",
        "        hidden = hidden.data\n",
        "\n",
        "        # calculate the loss\n",
        "        loss = criterion(prediction, y_tensor)\n",
        "        # zero gradients\n",
        "        optimizer.zero_grad()\n",
        "        # perform backprop and update weights\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # display loss and predictions\n",
        "        if batch_i%print_every == 0:  \n",
        "            print (batch_i)      \n",
        "            print('Loss: ', loss.item())\n",
        "            print ('Predicted Value: ', prediction.data.numpy().flatten())\n",
        "            print ('True Value: ', y_tensor.data.numpy().flatten())\n",
        "            \n",
        "    \n",
        "    return rnn,prediction[-1]\n",
        "\n",
        "# train the rnn and monitor results\n",
        "trained_rnn,final_prediction = train(rnn, n_steps = 75, print_every= 11)\n",
        "#print(final_prediction.data.numpy())\n",
        "print ('Final predicted value of input time series at t=6: ',final_prediction.item())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Loss:  1.5117348432540894\n",
            "Predicted Value:  [-0.18635052 -0.00746428  0.05009793  0.05678663 -0.02306957]\n",
            "True Value:  [-0.6117564  -0.5281718  -1.0729686   0.86540765 -2.3015387 ]\n",
            "11\n",
            "Loss:  0.17117467522621155\n",
            "Predicted Value:  [-1.1242571   0.01409128 -1.4364806   1.2616227  -2.4017377 ]\n",
            "True Value:  [-0.6117564  -0.5281718  -1.0729686   0.86540765 -2.3015387 ]\n",
            "22\n",
            "Loss:  0.0257004052400589\n",
            "Predicted Value:  [-0.8625432  -0.65644956 -1.2690521   0.9668686  -2.3217802 ]\n",
            "True Value:  [-0.6117564  -0.5281718  -1.0729686   0.86540765 -2.3015387 ]\n",
            "33\n",
            "Loss:  0.00878409668803215\n",
            "Predicted Value:  [-0.53372186 -0.4725641  -0.92941976  0.7992105  -2.2027938 ]\n",
            "True Value:  [-0.6117564  -0.5281718  -1.0729686   0.86540765 -2.3015387 ]\n",
            "44\n",
            "Loss:  0.006239252630621195\n",
            "Predicted Value:  [-0.53502613 -0.5857098  -1.0279086   0.72779965 -2.3336594 ]\n",
            "True Value:  [-0.6117564  -0.5281718  -1.0729686   0.86540765 -2.3015387 ]\n",
            "55\n",
            "Loss:  0.0010371545795351267\n",
            "Predicted Value:  [-0.596573   -0.5360092  -1.024904    0.86010087 -2.2509873 ]\n",
            "True Value:  [-0.6117564  -0.5281718  -1.0729686   0.86540765 -2.3015387 ]\n",
            "66\n",
            "Loss:  0.000279710948234424\n",
            "Predicted Value:  [-0.64293253 -0.52804244 -1.0787239   0.85450125 -2.3181071 ]\n",
            "True Value:  [-0.6117564  -0.5281718  -1.0729686   0.86540765 -2.3015387 ]\n",
            "Final predicted value of input time series at t=6:  -2.292928457260132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zfAVRvIFjxh"
      },
      "source": [
        "## Question 2:\n",
        "\n",
        "Given a Multivariate input time sequence and all the trainable parameters of LSTM Cell; Implement all the functionalities of the LSTM cell in order to predict the hidden state and output at time=t; given LSTM \"cell state\" at previous time step (t= t-1), LSTM \"hidden state\" at previous time step ( t= t-1) and the input at time=t. Hint : Follow the following sets of equation for implementing the functionality of LSTM Cell.\n",
        "\n",
        "Forget GATE: $f_{t} = \\sigma(W_{f}[ a_{t-1} ; x_{t}]  + b_{f}) $ (Note that \";\" denotes contatenation operation.)\n",
        "\n",
        "Update GATE: $i_{t} = \\sigma(W_{i}[ a_{t-1} ; x_{t} ] + b_i )$\n",
        "\n",
        "Memory GATE: $\\tilde{c}_{t} = tanh(W_c[ a_{t-1} ; x_{t} ] + b_c )$\n",
        "            update step -> $c_{t} =  f_{t} * c_{t-1} + i_{t} * \\tilde{c}_{t}$  (This operation determines how much information to keep from past and how much to add from current step information)\n",
        "\n",
        "Output GATE: $o_{t} = \\sigma(W_o [ a_{t-1} ; x_{t} ] + b_o)$\n",
        "           Final Output: $a_{t} = o_{t}*tanh(c_t) $\n",
        "( Note: For implementing \"tanh\" operation; use numpy.tanh libary function)\n",
        "\n",
        "\n",
        "a> Compute the value of a specific component of LSTM cell \"Output\" (y), i.e. y[1, 3, 4]? \n",
        "\n",
        "b> Also find the value of a specific component of LSTM hidden state output(a) ; i.e. a[2,1,5].\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOok6KSeaJ1o"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "np.random.seed(2)\n",
        "\n",
        "## Function implements Sigmoid Activation\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "## Function implements Softmax Activation\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "\n",
        "## Function implements LSTM \"forward pass\" of a single time step..i.e. given x at time step t, hidden state \n",
        "##at previous time step Memory state at previous time step , this function computes predicted output y at time step t. \n",
        "\n",
        "def lstm_forward_pass(xt, a_prev, c_prev, parameters):\n",
        "    \"\"\"\n",
        "    Implement a single forward step of the LSTM-cell \n",
        "\n",
        "    Arguments:\n",
        "    xt -- your input data at timestep \"t\"\n",
        "    a_prev -- Hidden state at timestep \"t-1\"\n",
        "    c_prev -- Memory state at timestep \"t-1\"\n",
        "\n",
        "    # Trainable Parameters of a LSTM cell\n",
        "    Wf -- Weight matrix of the forget gate; bf -- Bias of the forget gate\n",
        "    Wi -- Weight matrix of the update gate; bi -- Bias of the update gate\n",
        "    Wc -- Weight matrix of the first \"tanh\"; bc --  Bias of the first \"tanh\"\n",
        "    Wo -- Weight matrix of the output gate; bo --  Bias of the output gate\n",
        "    Wy -- Weight matrix relating the hidden-state to the output; by -- Bias relating the hidden-state to the output\n",
        "                        \n",
        "    The LSTM Cell MUST Returns:\n",
        "    a_next -- next hidden state\n",
        "    c_next -- next memory state\n",
        "    yt_pred -- LSTM output prediction at timestep \"t\"\n",
        "    cache -- tuple of values needed for the backward pass, contains (a_next, c_next, a_prev, c_prev, xt, parameters)\n",
        "    Note: ft/it/ot stand for the forget/update/output gates, cct stands for the candidate value (c tilde),\n",
        "          c stands for the memory value\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve parameters from \"parameters\"\n",
        "    Wf = parameters[\"Wf\"]; bf = parameters[\"bf\"]\n",
        "    Wi = parameters[\"Wi\"]; bi = parameters[\"bi\"]\n",
        "    Wc = parameters[\"Wc\"]; bc = parameters[\"bc\"]\n",
        "    Wo = parameters[\"Wo\"]; bo = parameters[\"bo\"]\n",
        "    Wy = parameters[\"Wy\"]; by = parameters[\"by\"]\n",
        "    \n",
        "    # Retrieve dimensions from shapes of xt and Wy\n",
        "    n_x, m = xt.shape\n",
        "    n_y, n_a = Wy.shape\n",
        "\n",
        "    ### YOUR CODE STARTS HERE ###\n",
        "    \n",
        "\n",
        "    ### YOUR CODE ENDS HERE ###\n",
        "\n",
        "    # store values needed for backward propagation in cache\n",
        "    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)\n",
        "\n",
        "    return a_next, c_next, yt_pred, cache\n",
        "\n",
        "def lstm_forward(x, a0, parameters):\n",
        "    \"\"\"\n",
        "    Implement the forward propagation of the recurrent neural network using an LSTM-cell.\n",
        "\n",
        "    Arguments:\n",
        "    x -- Input data for every time-step\n",
        "    a0 -- Initial hidden state of LSTM cell\n",
        "    parameters \n",
        "    Wf -- Weight matrix of the forget gate ;bf -- Bias of the forget gate\n",
        "    Wi -- Weight matrix of the update gate ;bi -- Bias of the update gate\n",
        "    Wc -- Weight matrix of the first \"tanh\";bc -- Bias of the first \"tanh\"\n",
        "    Wo -- Weight matrix of the output gate; bo -- Bias of the output gate\n",
        "    Wy -- Weight matrix relating the hidden-state to the output; by -- Bias relating the hidden-state to the output\n",
        "                        \n",
        "    This Function call MUST Returns:\n",
        "    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\n",
        "    y -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)\n",
        "    c -- Memory states for every time-step\n",
        "    caches -- tuple of values needed for the backward pass, contains (list of all the caches, x)\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize \"caches\", which will track the list of all the caches\n",
        "    caches = []\n",
        "    \n",
        "    ### YOUR CODE STARTS HERE ###\n",
        "\n",
        "\n",
        "    ### YOUR CODE ENDS HERE ###\n",
        "\n",
        "    # store values needed for backward propagation in cache\n",
        "    caches = (caches, x)\n",
        "    \n",
        "\n",
        "    return a, y, c, caches\n",
        "\n",
        "# Input time sequence\n",
        "x = np.random.randn(3,10,7)\n",
        "\n",
        "# Initial Hidden state of LSTM\n",
        "a0 = np.random.randn(5,10)\n",
        "\n",
        "# Weight and Bias Parameters of FORGET gate\n",
        "Weight_f = np.random.randn(5, 8); bias_f = np.random.randn(5,1)\n",
        "\n",
        "# Weight and Bias Parameters of UPDATE gate\n",
        "Weight_i = np.random.randn(5, 8); bias_i = np.random.randn(5,1)\n",
        "\n",
        "# Weight and Bias Parameters of OUTPUT gate\n",
        "Weight_o = np.random.randn(5, 8); bias_o = np.random.randn(5,1)\n",
        "\n",
        "# Weight and Bias Parameters of MEMORY gate (updating the cell)\n",
        "Weight_c = np.random.randn(5, 8); bias_c = np.random.randn(5,1)\n",
        "\n",
        "# Weight and bias for transforming hidden state output to final LSTM output for downstream application\n",
        "Weight_y = np.random.randn(2,5); bias_y = np.random.randn(2,1)\n",
        "\n",
        "LSTM_parameters = {\"Wf\": Weight_f, \"Wi\": Weight_i, \"Wo\": Weight_o, \"Wc\": Weight_c, \"Wy\": Weight_y, \"bf\": bias_f, \"bi\": bias_i, \"bo\": bias_o, \"bc\": bias_c, \"by\": bias_y}\n",
        "\n",
        "a, y, c, caches = lstm_forward(x, a0, LSTM_parameters)\n",
        "\n",
        "## Print the specific component value of LSTM cell \"Output\" (y) ;i.e. y[1,3,4]\n",
        "print(\"y[1][3][4] =\", y[1][3][4])\n",
        "print(\"y.shape = \", y.shape)\n",
        "\n",
        "## Print the specific component value of LSTM \"hidden state\" Output (a) ;i.e. a[2,1,5]\n",
        "print(\"a[2][1][5] = \", a[2][1][5])\n",
        "print(\"a.shape = \", a.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SQRxSU9Tkwx"
      },
      "source": [
        "## Question 3:\n",
        "\n",
        "Time series prediction using recurrent models. \n",
        "\n",
        "Use the airline-passengers.csv file for this task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e5Athrzw-DA"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "training_set = pd.read_csv('airline-passengers.csv')\n",
        "#training_set = pd.read_csv('shampoo.csv')\n",
        "\n",
        "training_set = training_set.iloc[:,1:2].values\n",
        "\n",
        "#plt.plot(training_set, label = 'Shampoo Sales Data')\n",
        "plt.plot(training_set, label = 'Airline Passangers Data')\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 432x288 with 1 Axes>",
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 375.2875 248.518125\" width=\"375.2875pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-03-22T14:50:31.866394</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 375.2875 248.518125 \nL 375.2875 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 33.2875 224.64 \nL 368.0875 224.64 \nL 368.0875 7.2 \nL 33.2875 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mc0183c22a6\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"48.505682\" xlink:href=\"#mc0183c22a6\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(45.324432 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"91.074023\" xlink:href=\"#mc0183c22a6\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20 -->\n      <g transform=\"translate(84.711523 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"133.642363\" xlink:href=\"#mc0183c22a6\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40 -->\n      <g transform=\"translate(127.279863 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"176.210704\" xlink:href=\"#mc0183c22a6\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60 -->\n      <g transform=\"translate(169.848204 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"218.779045\" xlink:href=\"#mc0183c22a6\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80 -->\n      <g transform=\"translate(212.416545 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"261.347386\" xlink:href=\"#mc0183c22a6\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100 -->\n      <g transform=\"translate(251.803636 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"303.915726\" xlink:href=\"#mc0183c22a6\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 120 -->\n      <g transform=\"translate(294.371976 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"346.484067\" xlink:href=\"#mc0183c22a6\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 140 -->\n      <g transform=\"translate(336.940317 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_9\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m1139e82824\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m1139e82824\" y=\"216.282794\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 100 -->\n      <g transform=\"translate(7.2 220.082013)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m1139e82824\" y=\"178.122036\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 200 -->\n      <g transform=\"translate(7.2 181.921255)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m1139e82824\" y=\"139.961278\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 300 -->\n      <g transform=\"translate(7.2 143.760496)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m1139e82824\" y=\"101.800519\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 400 -->\n      <g transform=\"translate(7.2 105.599738)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m1139e82824\" y=\"63.639761\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 500 -->\n      <g transform=\"translate(7.2 67.43898)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m1139e82824\" y=\"25.479003\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 600 -->\n      <g transform=\"translate(7.2 29.278222)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#p3693f45513)\" d=\"M 48.505682 211.703503 \nL 50.634099 209.413857 \nL 52.762516 204.071351 \nL 54.890933 205.216174 \nL 57.01935 208.269035 \nL 59.147767 202.926529 \nL 61.276184 197.96563 \nL 63.404601 197.96563 \nL 65.533018 202.544921 \nL 67.661435 209.03225 \nL 69.789852 214.756364 \nL 71.918269 209.413857 \nL 74.046686 210.55868 \nL 76.175103 206.360997 \nL 78.30352 200.636883 \nL 80.431937 202.926529 \nL 82.560354 206.742604 \nL 84.688771 197.584022 \nL 86.817188 189.570263 \nL 88.945606 189.570263 \nL 91.074023 194.149554 \nL 93.20244 203.689744 \nL 95.330857 210.940288 \nL 97.459274 201.018491 \nL 101.716108 197.202415 \nL 103.844525 186.517403 \nL 105.972942 192.241516 \nL 108.101359 188.807048 \nL 110.229776 186.517403 \nL 112.358193 178.503643 \nL 114.48661 178.503643 \nL 116.615027 184.227757 \nL 118.743444 192.623124 \nL 120.871861 198.728845 \nL 123.000278 191.096694 \nL 125.128695 189.188656 \nL 127.257112 185.754187 \nL 129.385529 180.793289 \nL 131.513946 185.37258 \nL 133.642363 184.609365 \nL 135.77078 171.253099 \nL 140.027614 162.094517 \nL 142.156031 174.687568 \nL 144.284449 181.556504 \nL 146.412866 188.807048 \nL 148.541283 180.411681 \nL 150.6697 179.648466 \nL 152.798117 179.648466 \nL 154.926534 164.384163 \nL 157.054951 164.76577 \nL 159.183368 167.055416 \nL 161.311785 161.71291 \nL 163.440202 153.699151 \nL 165.568619 150.64629 \nL 167.697036 164.002555 \nL 169.825453 173.924352 \nL 171.95387 185.754187 \nL 174.082287 177.740428 \nL 176.210704 176.595605 \nL 178.339121 182.701327 \nL 180.467538 164.76577 \nL 182.595955 167.818631 \nL 184.724372 165.147378 \nL 186.852789 153.699151 \nL 188.981206 139.198062 \nL 191.109623 142.632531 \nL 193.23804 155.607188 \nL 195.366457 167.055416 \nL 197.494874 176.977213 \nL 199.623291 167.055416 \nL 201.751709 162.094517 \nL 203.880126 165.528986 \nL 206.008543 152.554328 \nL 208.13696 151.791113 \nL 210.265377 151.409505 \nL 212.393794 134.237164 \nL 214.522211 115.538392 \nL 216.650628 122.025721 \nL 218.779045 135.381987 \nL 223.035879 164.002555 \nL 225.164296 148.356644 \nL 227.292713 146.066999 \nL 229.42113 148.738252 \nL 231.549547 133.473949 \nL 233.677964 135.000379 \nL 235.806381 133.092341 \nL 237.934798 111.722317 \nL 240.063215 96.839621 \nL 242.191632 99.892482 \nL 246.448466 137.671632 \nL 248.576883 151.027898 \nL 250.7053 137.671632 \nL 252.833717 134.237164 \nL 254.962134 139.57967 \nL 257.090551 118.591253 \nL 259.218969 121.644114 \nL 261.347386 118.972861 \nL 263.475803 93.405153 \nL 265.60422 76.996027 \nL 267.732637 76.232812 \nL 269.861054 100.274089 \nL 271.989471 122.025721 \nL 274.117888 138.05324 \nL 276.246305 126.223405 \nL 278.374722 124.696974 \nL 280.503139 133.092341 \nL 282.631556 116.301608 \nL 284.759973 121.644114 \nL 286.88839 115.92 \nL 289.016807 88.444254 \nL 291.145224 67.07423 \nL 293.273641 61.731723 \nL 295.402058 100.274089 \nL 297.530475 117.44643 \nL 299.658892 136.145202 \nL 301.787309 125.841797 \nL 303.915726 117.064823 \nL 306.044143 123.933759 \nL 308.17256 99.510874 \nL 310.300977 103.32695 \nL 312.429394 94.168368 \nL 314.557812 74.324774 \nL 316.686229 45.322597 \nL 318.814646 41.124914 \nL 320.943063 77.759242 \nL 323.07148 99.129266 \nL 325.199897 116.301608 \nL 327.328314 99.892482 \nL 329.456731 95.313191 \nL 331.585148 105.234988 \nL 333.713565 94.549975 \nL 335.841982 78.522457 \nL 337.970399 74.324774 \nL 340.098816 50.283496 \nL 342.227233 17.083636 \nL 344.35565 23.189358 \nL 346.484067 60.586901 \nL 348.612484 78.522457 \nL 350.740901 105.616595 \nL 352.869318 89.589077 \nL 352.869318 89.589077 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 33.2875 224.64 \nL 33.2875 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 368.0875 224.64 \nL 368.0875 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 33.2875 224.64 \nL 368.0875 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 33.2875 7.2 \nL 368.0875 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p3693f45513\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"33.2875\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qo-Bn3vxGvD"
      },
      "source": [
        "## Prepare the training and testing dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCFod5yYxQXt"
      },
      "source": [
        "def sliding_windows(data, seq_length):\n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    for i in range(len(data)-seq_length-1):\n",
        "        _x = data[i:(i+seq_length)]\n",
        "        _y = data[i+seq_length]\n",
        "        x.append(_x)\n",
        "        y.append(_y)\n",
        "\n",
        "    return np.array(x),np.array(y)\n",
        "\n",
        "sc = MinMaxScaler()\n",
        "training_data = sc.fit_transform(training_set)\n",
        "\n",
        "seq_length = 4\n",
        "x, y = sliding_windows(training_data, seq_length)\n",
        "\n",
        "train_size = int(len(y) * 0.67)\n",
        "test_size = len(y) - train_size\n",
        "\n",
        "dataX = Variable(torch.Tensor(np.array(x)))\n",
        "dataY = Variable(torch.Tensor(np.array(y)))\n",
        "\n",
        "trainX = Variable(torch.Tensor(np.array(x[0:train_size])))\n",
        "trainY = Variable(torch.Tensor(np.array(y[0:train_size])))\n",
        "\n",
        "testX = Variable(torch.Tensor(np.array(x[train_size:len(x)])))\n",
        "testY = Variable(torch.Tensor(np.array(y[train_size:len(y)])))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-fbsv6_xVhu"
      },
      "source": [
        "## Create the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KC_5L1NIxUsR"
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
        "        super(LSTM, self).__init__()\n",
        "        \n",
        "        self.num_classes = num_classes\n",
        "        self.num_layers = num_layers\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "        # Your code goes here\n",
        "        self.lstm   = nn.LSTM(input_size,hidden_size, num_layers,batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "        # define a lstm block and a fc block\n",
        "\n",
        "    def forward(self, x):\n",
        "       \n",
        "        # Your code goes here\n",
        "        out, h_out = self.lstm(x)\n",
        "        out = out.reshape(-1, hidden_size) \n",
        "        out = self.fc(out)\n",
        "        # Define a forward function for the LSTM block\n",
        "        return out"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQKOnThFxxuJ"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlj9F1Z6xxFR"
      },
      "source": [
        "num_epochs = 2000\n",
        "learning_rate = 0.01\n",
        "\n",
        "input_size = 1\n",
        "hidden_size = 2\n",
        "num_layers = 1\n",
        "\n",
        "num_classes = 1\n",
        "\n",
        "lstm = LSTM(num_classes, input_size, hidden_size, num_layers)\n",
        "\n",
        "criterion = torch.nn.MSELoss()    # mean-squared error for regression\n",
        "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
        "#optimizer = torch.optim.SGD(lstm.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # Your code goes here\n",
        "    for i in range(train_size):\n",
        "        x=trainX[i]\n",
        "        y=trainY[i]\n",
        "        x = x.unsqueeze(0) # unsqueeze gives a 1, batch_size dimension\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = lstm(x)\n",
        "\n",
        "        loss = criterion(y_pred,y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    # define a training iteration to the lstm model. Use loss variable to compute the training loss\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "      print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'int' object is not iterable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-1808a1997dc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Your code goes here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gF0UsfR8yFrO"
      },
      "source": [
        "## Test and Visualize the results. \n",
        "Plot the output of time series similar to the data plot above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7-eERYByJ3O"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPTjxV0G6g8X"
      },
      "source": [
        "#### **Welcome to Assignment 4 (part-2) on Deep Learning for Computer Vision.**\n",
        "This question consists of two subsection. In subsection-1 you'll have to code a Siamese Network, for subsection-2 you need to go through a official PyTorch tutorial on Object Detection, understand it and answer some questions.\n",
        "  \n",
        "#### **Instructions**\n",
        "1. Use Python 3.x to run this notebook\n",
        "2. Write your code only in between the lines 'YOUR CODE STARTS HERE' and 'YOUR CODE ENDS HERE'.\n",
        "you should not change anything else in the code cells, if you do, the answers you are supposed to get at the end of this assignment might be wrong.\n",
        "3. Read documentation of each function carefully.\n",
        "4. All the Best!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJwH6jxrqI-5"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.sampler import BatchSampler\n",
        "from torch.optim import lr_scheduler\n",
        "from PIL import Image\n",
        "import timeit\n",
        "\n",
        "## Please DONOT remove these lines. \n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(0)\n",
        "########################\n",
        "\n",
        "#### YOUR CODE STARTS HERE ####\n",
        "# Check availability of GPU and set the device accordingly\n",
        "device = \n",
        "#### YOUR CODE ENDS HERE ####\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhSvcqdYqJ6U"
      },
      "source": [
        "#### Prepare the dataset for Siamese Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stYbGPoLqzDE"
      },
      "source": [
        "class SiameseDataset(Dataset):\n",
        "    def __init__(self, train=True):\n",
        "        \n",
        "        self.train = train\n",
        "        #### YOUR CODE STARTS HERE ####\n",
        "        # Define a set of transforms for preparing the dataset\n",
        "        self.transform =  # convert the image to a pytorch tensor\n",
        "                          # normalise the images with mean and std of the dataset\n",
        "        \n",
        "        # Load the MNIST training, test datasets using `torchvision.datasets.MNIST\n",
        "        # Set the train parameter to self.train and transform parameter to self.transform\n",
        "        self.dataset = \n",
        "\n",
        "        #### YOUR CODE ENDS HERE ####\n",
        "        if self.train:\n",
        "            #### YOUR CODE STARTS HERE ####\n",
        "            # assign input (x-values) of training data \n",
        "            self.train_data =\n",
        "            # assign labels of training data \n",
        "            self.train_labels = \n",
        "            # get the set of all the labels in the dataset\n",
        "            self.labels_all = \n",
        "            self.label_to_idx = {} # assign a unique index to all labels in the dataset and store them in a dictionary \n",
        "\n",
        "            #### YOUR CODE ENDS HERE ####\n",
        "        else:\n",
        "            #### YOUR CODE STARTS HERE ####\n",
        "            # assign input (x-values) of test data \n",
        "            self.test_data = \n",
        "            # assign labels of test data \n",
        "            self.test_labels = \n",
        "            # get the set of all the labels in the dataset\n",
        "            self.labels_all = \n",
        "            self.label_to_idx = {} # assign a unique index to all labels in the dataset and store them in a dictionary \n",
        "\n",
        "            #### YOUR CODE ENDS HERE ####\n",
        "            # DONOT change this line of code  \n",
        "            random_state = np.random.RandomState(0)\n",
        "\n",
        "            positive_samples = [] # this will be a list of lists\n",
        "            for ind in range(0, len(self.test_data), 2):\n",
        "              positive_samples.append([ind, random_state.choice(self.label_to_idx[self.test_labels[ind].item()]), 1])\n",
        "            \n",
        "            negative_samples = []\n",
        "            for ind in range(1, len(self.test_data), 2):\n",
        "              negative_samples.append([ind, random_state.choice(self.label_to_idx[np.random.choice(\n",
        "                                                           list(self.labels_all - set([self.test_labels[ind].item()])))]), 0])\n",
        "            \n",
        "            # combine both positive and negative samples into a single variable\n",
        "            #### YOUR CODE STARTS HERE ####\n",
        "            self.test_samples = \n",
        "            #### YOUR CODE ENDS HERE ####\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # get image pair, and the associated label (1 for similar, 0 for dissimilar)\n",
        "        if self.train:\n",
        "            #### YOUR CODE STARTS HERE ####\n",
        "            # choose if training sample is similar or dissimilar\n",
        "            target = \n",
        "            # choose two images call them `first_image` and `second_image`\n",
        "            \n",
        "            #### YOUR CODE ENDS HERE ####\n",
        "        else:\n",
        "            #### YOUR CODE STARTS HERE ####\n",
        "            # select two images from self.test_samples, call them `first_image` and `second_image`\n",
        "            first_image = \n",
        "            second_image = \n",
        "            target = \n",
        "            #### YOUR CODE ENDS HERE ####\n",
        "        \n",
        "        first_image = Image.fromarray(first_image.numpy(), mode='L')\n",
        "        second_image = Image.fromarray(second_image.numpy(), mode='L')\n",
        "        first_image = self.transform(first_image)\n",
        "        second_image = self.transform(second_image)\n",
        "        return (first_image, second_image), target\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gEE-dEarnvg"
      },
      "source": [
        "class EmbeddingNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EmbeddingNet, self).__init__()\n",
        "        #### YOUR CODE STARTS HERE ####\n",
        "        # Define a sequential block as per the instructions below:\n",
        "        # Build three blocks with each block containing: Conv->PReLU->Maxpool layers\n",
        "        # Three conv layers should have 16, 32, 64 output channels respectively\n",
        "        # Use convolution kernel size 3\n",
        "        # For maxpool use a kernel size of 2 and stride of 2\n",
        "\n",
        "        self.convnet = \n",
        "\n",
        "\n",
        "        # Define linear->PReLU->linear->PReLU->linear\n",
        "        # The first two linear layers should have 256 and 128 output nodes\n",
        "        # The final FC layer should have 2 nodes\n",
        "        self.fc =\n",
        "\n",
        "        #### YOUR CODE ENDS HERE ####\n",
        "\n",
        "    def forward(self, x):\n",
        "      #### YOUR CODE STARTS HERE ####\n",
        "        # Define the forward pass, convnet -> fc\n",
        "        output = \n",
        "        #### YOUR CODE ENDS HERE ####\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPIClNjsrz78"
      },
      "source": [
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self, embedding_net):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        self.embedding_net = embedding_net\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        # Call the embedding network for both the inputs and return the output\n",
        "        #### YOUR CODE ENDS HERE ####\n",
        "        op1 = \n",
        "        op2 = \n",
        "        #### YOUR CODE ENDS HERE ####\n",
        "        return op1, op2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzlZzVgmM3hC"
      },
      "source": [
        "Pairwise contrastive loss\n",
        "$$\n",
        "L\\left(x_{0}, x_{1}, y\\right)=\\frac{1}{2} y\\left\\|f\\left(x_{0}\\right)-f\\left(x_{1}\\right)\\right\\|_{2}^{2}+\\frac{1}{2}(1-y)\\left\\{\\max (0, m-\\sqrt{\\|f(x_{0})-f(x_{1})\\|_{2}^{2} + \\epsilon)})\\right\\}^{2}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BclsdWZSr4RK"
      },
      "source": [
        "class ContrastiveLossSiamese(nn.Module):\n",
        "\n",
        "    def __init__(self, margin):\n",
        "        super(ContrastiveLossSiamese, self).__init__()\n",
        "        self.margin = margin\n",
        "        self.eps = 1e-9\n",
        "\n",
        "    def forward(self, output1, output2, target):\n",
        "        # Use the equation mentioned above to define the loss\n",
        "        #### YOUR CODE STARTS HERE ####\n",
        "        loss_value = \n",
        "        #### YOUR CODE ENDS HERE ####\n",
        "        loss_value = loss_value.mean()\n",
        "\n",
        "        return loss_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVrUkFLmca1I"
      },
      "source": [
        "def train(model, train_loader, device, optimizer, criterion, epoch):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        target = target if len(target) > 0 else None\n",
        "        #### YOUR CODE STARTS HERE ####\n",
        "        # send the image, target to the device\n",
        "        # data is not a single value here,\n",
        "        # ensure datatype of variable `data` is tuple\n",
        "        data = \n",
        "        target = \n",
        "        # flush out the gradients stored in optimizer\n",
        "\n",
        "        # pass the image to the model and assign the output to variable named outputs\n",
        "        # python star operator will be useful here\n",
        "        # if the datatype of outputs is not a tuple, make it to a tuple\n",
        "\n",
        "        outputs = \n",
        "\n",
        "        # create inputs to the contrastive loss (datatype should be tuple)\n",
        "        # calculate the loss using criterion \n",
        "        loss = \n",
        "        # append the loss to losses list and update the total_loss variable\n",
        "\n",
        "        # do a backward pass\n",
        "\n",
        "        # update the weights\n",
        "\n",
        "        #### YOUR CODE ENDS HERE ####\n",
        "\n",
        "        if batch_idx % 20 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data[0]), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), np.mean(losses)))  \n",
        "    total_loss /= (batch_idx + 1)\n",
        "    print('Average loss on training set: {:.6f}'.format(total_loss))\n",
        "\n",
        "def test(model, test_loader, device, criterion):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(test_loader):\n",
        "          target = target if len(target) > 0 else None\n",
        "          #### YOUR CODE STARTS HERE ####\n",
        "          # send the image, target to the device\n",
        "          # data is not a single value here,\n",
        "          # ensure datatype of variable `data` is tuple\n",
        "          data = \n",
        "          target = \n",
        "          # pass the image to the model and assign the output to variable named outputs\n",
        "          # python star operator will be useful here\n",
        "          # if the datatype of outputs is not a tuple, make it to a tuple\n",
        "          outputs =\n",
        "\n",
        "          # create inputs to the contrastive loss\n",
        "          # datatype of target should be tuple\n",
        "          # calculate the loss\n",
        "          loss = \n",
        "          # update the test_loss variable\n",
        "          \n",
        "          #### YOUR CODE ENDS HERE ####\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    print('Average loss on test set: {:.6f}'.format(test_loss))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDZAVVWVcAC5"
      },
      "source": [
        "# define the training and test sets\n",
        "# use SiameseDataset\n",
        "train_dataset = \n",
        "test_dataset =\n",
        "\n",
        "# create dataloaders for training and test datasets\n",
        "# use a batch size of 128 and set shuffle=True for the training set, set num_workers to 2 and pin_memory to True\n",
        "train_dataloader = \n",
        "test_dataloader = \n",
        "\n",
        "margin = 1.\n",
        "# create a instance of the embedding network and pass it as input to Siamese network\n",
        "embedding_net = \n",
        "model = \n",
        "# define the contrative loss with the specified margin\n",
        "criterion = \n",
        "optimizer = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CejgunVGzJPK"
      },
      "source": [
        "start = timeit.default_timer()\n",
        "for epoch in range(1, 5):\n",
        "  train(model, train_dataloader, device, optimizer, criterion, epoch)\n",
        "  test(model, test_dataloader, device, criterion)\n",
        "\n",
        "stop = timeit.default_timer()\n",
        "print('Total time taken: {} seconds'.format(int(stop - start)) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S0vb38a_o_r"
      },
      "source": [
        "### Question 4\n",
        "\n",
        "Run the code cell above and plot(on the same graph) the average train and test losses w.r.t epochs trained."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgyN14PN5npM"
      },
      "source": [
        "### Question 5\n",
        "\n",
        "We have looked at training a Siamese Network with a pairwise contrastive loss. How would you modify the architecture above to use a triplet loss? The triplet loss is a ranking loss that uses image triplets consisting of an anchor $x_a$, a positive sample(similar to the anchor image) $x_p$ and a negative sample(dissimilar to the anchor image) $x_n$. Given triplet $(x_a, x_p, x_n)$ the teiplet loss is:\n",
        "\n",
        "$$\n",
        "L\\left(x_{a}, x_{p}, x_{n}\\right)=\\max \\left(0, m + \\|f(x_{a})-f(x_{p})\\|_{2}^{2} - \\|f(x_{a})-f(x_{n})\\|_{2}^{2}\\right)\n",
        "$$\n",
        "\n",
        "1. Write the dataloader to get triplets.\n",
        "2. Write code for class TripletLossSiamese. (the triplet loss)\n",
        "3. Describe in words how would you modify the network architecture to train it with a triplet loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1UhDPNO5npM"
      },
      "source": [
        "# Question 2.1, dataloader for triplets\n",
        "class TripletDataset(Dataset):\n",
        "    def __init__(self, train=True):\n",
        "        \n",
        "        self.train = train\n",
        "        #### YOUR CODE STARTS HERE ####\n",
        "        # Define a set of transforms for preparing the dataset\n",
        "        self.transform =  # convert the image to a pytorch tensor\n",
        "                          # normalise the images with mean and std of the dataset\n",
        "        \n",
        "        # Load the MNIST training, test datasets using `torchvision.datasets.MNIST\n",
        "        # Set the train parameter to self.train and transform parameter to self.transform\n",
        "        self.dataset = \n",
        "\n",
        "        #### YOUR CODE ENDS HERE ####\n",
        "        if self.train:\n",
        "            #### YOUR CODE STARTS HERE ####\n",
        "            # assign input (x-values) of training data \n",
        "            self.train_data =\n",
        "            # assign labels of training data \n",
        "            self.train_labels = \n",
        "            # get the set of all the labels in the dataset\n",
        "            self.labels_all = \n",
        "            self.label_to_idx = {} # assign a unique index to all labels in the dataset and store them in a dictionary \n",
        "\n",
        "            #### YOUR CODE ENDS HERE ####\n",
        "        else:\n",
        "            #### YOUR CODE STARTS HERE ####\n",
        "            # assign input (x-values) of test data \n",
        "            self.test_data = \n",
        "            # assign labels of test data \n",
        "            self.test_labels = \n",
        "            # get the set of all the labels in the dataset\n",
        "            self.labels_all = \n",
        "            self.label_to_idx = {} # assign a unique index to all labels in the dataset and store them in a dictionary \n",
        "\n",
        "            #### YOUR CODE ENDS HERE ####\n",
        "            # DONOT change this line of code  \n",
        "            random_state = np.random.RandomState(0)\n",
        "            triplets = [] #this will be list of lists\n",
        "            for i in range(len(self.test_data)):\n",
        "                triplets.append([i, random_state.choice(self.label_to_idx[self.test_labels[i].item()]),\n",
        "                 random_state.choice(self.label_to_idx[np.random.choice(list(self.labels_all - set([self.test_labels[i].item()])))])\n",
        "                                ])\n",
        "            self.test_samples = triplets\n",
        "\n",
        "           \n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # get image triplet\n",
        "        if self.train:\n",
        "            anchor, anchor_label = self.train_data[index], self.train_labels[index].item\n",
        "            #### YOUR CODE STARTS HERE ####\n",
        "            #choose positive and negative image call them `positive` and `negative` respectively\n",
        "            \n",
        "            #### YOUR CODE ENDS HERE ####\n",
        "        else:\n",
        "            #### YOUR CODE STARTS HERE ####\n",
        "            # select three images from self.test_samples\n",
        "            anchor = \n",
        "            positive = \n",
        "            negative = \n",
        "            #### YOUR CODE ENDS HERE ####\n",
        "        \n",
        "        anchor = Image.fromarray(anchor.numpy(), mode='L')\n",
        "        positive = Image.fromarray(positive.numpy(), mode='L')\n",
        "        negative = Image.fromarray(negative.numpy(), mode='L')\n",
        "        anchor = self.transform(anchor)\n",
        "        positive = self.transform(positive)\n",
        "        negative = self.transform(negative)\n",
        "        return (anchor, positive, negative), []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yz-LGiJN5npM"
      },
      "source": [
        "# Question 2.2, define triplet loss\n",
        "class TripletLossSiamese(nn.Module):\n",
        "\n",
        "    def __init__(self, margin):\n",
        "        super(TripletLossSiamese, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, output_anchor, output_positive, output_negative):\n",
        "        # Use the triplet loss equation mentioned above to define the loss\n",
        "        #### YOUR CODE STARTS HERE ####\n",
        "        loss_value = \n",
        "        #### YOUR CODE ENDS HERE ####\n",
        "        loss_value = loss_value.mean()\n",
        "\n",
        "        return loss_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oP275cQFPvA"
      },
      "source": [
        "## Object Detection\n",
        "\n",
        "Go through the [Torchvision Object Detection Tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html) and ensure you understand the tutorial completely!\n",
        "\n",
        "After you have completely gone through the tutorial answer the following questions!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtaG0iORIXid"
      },
      "source": [
        "### Question 6\n",
        "\n",
        "Consider the metrics `AP@IoU=0.5` and `AP@IoU=0.75` used in the tutorial. Which of the following statements is True?  \n",
        "\n",
        "1. `IoU@0.75` will always be less than `IoU@0.5`\n",
        "2. `IoU@0.75` will always be  greater than `IoU@0.5` \n",
        "3. `IoU@0.75` need not be always be less than `IoU@0.5`\n",
        "4. `IoU@0.75` need not always be  greater than `IoU@0.5` "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9yyqsuyPAMv"
      },
      "source": [
        "### Question 7\n",
        "\n",
        "Tutorial uses a network that is pre-trained on COCO dataset. Will training this model from scratch improve the performance? Provide justification for your answer. (Hint: You don't really have to re-train the model for this)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71DVEZQf5hr8"
      },
      "source": [
        "### Question 8\n",
        "\n",
        "Write code to calculate IoU between aligned predicted bounding-boxes bbox_p and ground-truth bounding-boxes bbox_gt. Assume a co-ordinate system that has origin (0,0) at the upper-left corner of the image, and to the  right and down are +ve directions of x-axis and y-axis respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rngaSEln5npN"
      },
      "source": [
        "def calculate_iou(bbox_p, bbox_gt):\n",
        "    #input: bbox_p and bbox_gt are (N,4) tensors\n",
        "    #output: ious (N,) vector\n",
        "    N = bbox_p.size(0)\n",
        "    \n",
        "    #### YOUR CODE STARTS HERE ####\n",
        "    # write code to compute the IoU between the bounding boxes\n",
        "    \n",
        "    #### YOUR CODE ENDS HERE ####\n",
        "    \n",
        "    return ious"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR10OiXy6SkR"
      },
      "source": []
    }
  ]
}